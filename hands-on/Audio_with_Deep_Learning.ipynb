{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamod/BA865-2024/blob/main/hands-on/Audio_with_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you will learn how to deal with audio data."
      ],
      "metadata": {
        "id": "_NeFG4VN5scR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrWNC6CiH7Dd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa   #for audio processing\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Downloading"
      ],
      "metadata": {
        "id": "G9RLzr0efzg7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuKhes1QLe3b"
      },
      "source": [
        "Next, we'll download and unzip the dataset of speech commands from tensorflow: http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ve2s44HL6G5"
      },
      "source": [
        "Below we load the data and print the number of examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W--i8r_iwMv-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def getData(sampling=16000, use_MFCC=True, n_mfcc=12, number_of_samples={'yes':2000, 'no': 2000}, noise=.0):\n",
        "  #download the data\n",
        "  my_file = Path(\"/content/speech_commands/\")\n",
        "  if not my_file.exists():\n",
        "    !wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
        "    !mkdir speech_commands\n",
        "    !tar -C ./speech_commands -xf speech_commands_v0.01.tar.gz\n",
        "\n",
        "  directory = 'speech_commands/'\n",
        "  all_wavs = [] # The inputs\n",
        "  all_labs = [] # The outputs\n",
        "\n",
        "  # create the dataset as arrays\n",
        "  for label in number_of_samples.keys():\n",
        "      wavs = [f for f in os.listdir(directory + label) if f.endswith('.wav')]\n",
        "      for indx, wav in enumerate(wavs):\n",
        "          samples, sample_rate = librosa.load(directory + label + '/' + wav, sr = 16000)\n",
        "          samples = librosa.resample(samples, orig_sr=16000, target_sr=sampling) # Resamples the audio to possibly lower sampling rate: https://librosa.org/doc/main/generated/librosa.resample.html\n",
        "\n",
        "          if noise != 0:\n",
        "            samples = samples+noise*np.random.randn(*samples.shape)\n",
        "            samples = samples/max(samples)\n",
        "\n",
        "          if number_of_samples[label] > indx:\n",
        "            if(len(samples)== sampling): # makes sure all samples have the same length\n",
        "                all_wavs.append(samples)\n",
        "                all_labs.append(label)\n",
        "\n",
        "  # applying MFCC\n",
        "  if use_MFCC:\n",
        "    all_wavs = librosa.feature.mfcc(y=np.array(all_wavs), sr=sampling, n_mfcc=n_mfcc)\n",
        "  else:\n",
        "    all_wavs = np.array(all_wavs)\n",
        "\n",
        "  all_wavs = torch.tensor(all_wavs).float()\n",
        "  if not use_MFCC:\n",
        "    all_wavs = all_wavs.unsqueeze(-1)\n",
        "\n",
        "  # Output encoding\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  le = LabelEncoder()\n",
        "  all_labs_encoded = le.fit_transform(all_labs)\n",
        "  all_labs_encoded = torch.LongTensor(all_labs_encoded)\n",
        "\n",
        "  print(\"Total number of samples:\", len(all_wavs))\n",
        "\n",
        "\n",
        "\n",
        "  return all_wavs, all_labs_encoded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Some data parameters\n",
        "\n",
        "# Sampling (lower means less crisp audio)\n",
        "sampling = 16000 # Sampling rate (e.g., 16000)\n",
        "\n",
        "# MelSpectogram params: convert raw data to frequency domain\n",
        "use_MFCC = False\n",
        "n_mfcc = 12 # input audio data size (higher -> more crisp sound) (e.g. 12)\n",
        "\n",
        "# Other data params\n",
        "number_of_samples = {'yes':2000, 'no': 2000} # We will try to classify between two classes. This defines the number of samples per class.\n",
        "noise = 0. # If not zero, it makes the audio proportionally noisier (range: 0-1)"
      ],
      "metadata": {
        "id": "m95VlOgmhuta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_data, audio_label = getData(sampling=sampling, use_MFCC=use_MFCC, n_mfcc=n_mfcc, number_of_samples=number_of_samples, noise=noise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psegm255h0S4",
        "outputId": "6c3c282f-eaef-4d5f-c5e4-8359010f51cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples: 3591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_data.shape"
      ],
      "metadata": {
        "id": "JgHn5oNWQ98E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Playing an example"
      ],
      "metadata": {
        "id": "TwBNZm9sjqHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indx_to_play = 0\n",
        "\n",
        "# Only works if MFCC is not used\n",
        "print(audio_label[indx_to_play].item())\n",
        "ipd.Audio(audio_data[indx_to_play],rate=sampling,autoplay=True)"
      ],
      "metadata": {
        "id": "07nTwEDIkJJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model"
      ],
      "metadata": {
        "id": "UlP7n2lElCku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(dataloader, model):\n",
        "  acc = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for audios, labels in dataloader:\n",
        "          if use_cuda:\n",
        "            audios = audios.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "          outputs = model(audios)\n",
        "\n",
        "          # Update accuracy for this batch\n",
        "          acc = acc + torch.sum(torch.argmax(outputs, axis=1) == labels)\n",
        "\n",
        "\n",
        "      # Compute the accuracy\n",
        "      acc = acc/len(dataloader.dataset) # normalizes\n",
        "\n",
        "      return acc\n"
      ],
      "metadata": {
        "id": "QfscuLA3gqS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the RNN classification model\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes, use_hn=True):\n",
        "      ##############\n",
        "\n",
        "    def forward(self, x):\n",
        "      ###############"
      ],
      "metadata": {
        "id": "06JZIZFeIJcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "oyweDVCSm5Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "n4jUJYKIk8u7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next section, you will define your experiment's parameters and model's hyperparameters as flags here. Use these flags in your code so you can switch between experiemnts easily."
      ],
      "metadata": {
        "id": "iUhXt_QMmKkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# Data\n",
        "batch_size = 32\n",
        "\n",
        "# optimizer\n",
        "lr=0.0005\n",
        "epochs=100\n",
        "\n",
        "# model\n",
        "model_name = \"RNN\"\n",
        "hidden_size = 10 # hn for the RNN\n",
        "num_layers = 2 # Number of layers for RNN"
      ],
      "metadata": {
        "id": "dpIXwE4dk7ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vxsw0FHfZ2O",
        "outputId": "d903c062-0b49-414d-ae8a-603dab6fe27c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3591, 400, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "fnJHcf0jldqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "def getDataLoaders(audio_data, audio_label, batch_size):\n",
        "  # normalization\n",
        "  mean = audio_data.mean([0, 1]) if use_MFCC else audio_data.mean([0])\n",
        "  std = audio_data.std([0, 1]) if use_MFCC else audio_data.std([0])\n",
        "  audio_data_transformed = (audio_data - mean)/std\n",
        "\n",
        "\n",
        "  # Load the data loaders\n",
        "  my_dataset = torch.utils.data.TensorDataset(audio_data_transformed, audio_label)\n",
        "  train_set, val_set = torch.utils.data.random_split(my_dataset, [0.8, 0.2])\n",
        "\n",
        "  train_loader = DataLoader(dataset=train_set, batch_size=batch_size,  shuffle=True)\n",
        "  val_loader = DataLoader(dataset=val_set, batch_size=batch_size,  shuffle=False)\n",
        "\n",
        "  return train_loader, train_set, val_loader, val_set"
      ],
      "metadata": {
        "id": "LJw3mv-blhFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, train_set, val_loader, val_set = getDataLoaders(audio_data, audio_label, batch_size)"
      ],
      "metadata": {
        "id": "pzRjvKvFhusE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0][0].shape"
      ],
      "metadata": {
        "id": "msPwdpjyQUaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "5MEEKaylltRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchinfo"
      ],
      "metadata": {
        "id": "uJjkVm7etf9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchinfo import summary\n",
        "\n",
        "# Create an instance of the RNN classification model\n",
        "input_size = train_set[0][0].shape[-1]\n",
        "num_classes = 2\n",
        "if model_name == \"RNN\":\n",
        "  model = RNNClassifier(input_size, hidden_size, num_layers, num_classes)\n",
        "else:\n",
        "  raise \"Model not found\"\n",
        "print(model)\n",
        "print(summary(model, train_set[0][0].unsqueeze(0).shape))\n",
        "\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.cuda()\n",
        "\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for epoch in range(epochs): # The epochs.\n",
        "    for i, (audios, labels) in enumerate(train_loader): # The batches.\n",
        "        # step 1: Zero out the gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # step 1.1 move data to cuda. Make sure the model is on cuda too!\n",
        "        if use_cuda:\n",
        "          audios = audios.cuda()\n",
        "          labels = labels.cuda()\n",
        "\n",
        "        # print('labels', labels)\n",
        "\n",
        "\n",
        "        # step2: Forward pass\n",
        "        outputs = model(audios)\n",
        "\n",
        "        # print('outputs', outputs)\n",
        "\n",
        "        # step 3: calculate the loss.\n",
        "        loss = criterion(outputs, labels.view(-1))\n",
        "\n",
        "        # step 4: Backward pass\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "            # Print the loss\n",
        "        # if epoch %1 == 0 and i %10 == 0:\n",
        "        #   print(\"Epoch\", epoch+ 1, \" batch\", i+1, \". Training Loss: \", loss.item())\n",
        "\n",
        "            # Print the loss\n",
        "    if epoch %1 == 0:\n",
        "      print(\"Epoch\", epoch+ 1,  'acc: ', get_accuracy(train_loader,model).item(), 'val_acc: ', get_accuracy(val_loader,model).item())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sGjuLFl0TTYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction"
      ],
      "metadata": {
        "id": "HFV9Fuc1D-wP"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}