{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamod/BA865-2024/blob/main/hands-on/Audio_with_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you will learn how to deal with audio data."
      ],
      "metadata": {
        "id": "_NeFG4VN5scR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrWNC6CiH7Dd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa   #for audio processing\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Downloading"
      ],
      "metadata": {
        "id": "G9RLzr0efzg7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuKhes1QLe3b"
      },
      "source": [
        "Next, we'll download and unzip the dataset of speech commands from tensorflow: http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ve2s44HL6G5"
      },
      "source": [
        "Below we load the data and print the number of examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W--i8r_iwMv-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def getData(sampling=16000, use_MFCC=True, n_mfcc=12, number_of_samples={'yes':2000, 'no': 2000}, noise=.0, normalize=True):\n",
        "  #download the data\n",
        "  my_file = Path(\"/content/speech_commands/\")\n",
        "  if not my_file.exists():\n",
        "    !wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\n",
        "    !mkdir speech_commands\n",
        "    !tar -C ./speech_commands -xf speech_commands_v0.01.tar.gz\n",
        "\n",
        "  directory = 'speech_commands/'\n",
        "  all_wavs = [] # The inputs\n",
        "  all_labs = [] # The outputs\n",
        "\n",
        "  # create the dataset as arrays\n",
        "  for label in number_of_samples.keys():\n",
        "      wavs = [f for f in os.listdir(directory + label) if f.endswith('.wav')]\n",
        "      for indx, wav in enumerate(wavs):\n",
        "          samples, sample_rate = librosa.load(directory + label + '/' + wav, sr = 16000)\n",
        "          samples = librosa.resample(samples, orig_sr=16000, target_sr=sampling) # Resamples the audio to possibly lower sampling rate: https://librosa.org/doc/main/generated/librosa.resample.html\n",
        "\n",
        "          if noise != 0:\n",
        "            samples = samples+noise*np.random.randn(*samples.shape)\n",
        "            samples = samples/max(samples)\n",
        "\n",
        "          if number_of_samples[label] > indx:\n",
        "            if(len(samples)== sampling): # makes sure all samples have the same length\n",
        "                all_wavs.append(samples)\n",
        "                all_labs.append(label)\n",
        "\n",
        "  # applying MFCC\n",
        "  if use_MFCC:\n",
        "    all_wavs = librosa.feature.mfcc(y=np.array(all_wavs), sr=sampling, n_mfcc=n_mfcc)\n",
        "  else:\n",
        "    all_wavs = np.array(all_wavs)\n",
        "\n",
        "  all_wavs = torch.tensor(all_wavs).float()\n",
        "  if not use_MFCC:\n",
        "    all_wavs = all_wavs.unsqueeze(-1)\n",
        "\n",
        "  # Output encoding\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  le = LabelEncoder()\n",
        "  all_labs_encoded = le.fit_transform(all_labs)\n",
        "  all_labs_encoded = torch.LongTensor(all_labs_encoded)\n",
        "\n",
        "  print(\"Total number of samples:\", len(all_wavs))\n",
        "\n",
        "  return all_wavs, all_labs_encoded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Some data parameters\n",
        "\n",
        "# Sampling (lower means less crisp audio)\n",
        "sampling = 400 # Sampling rate (e.g., 16000). Smaller (e.g., 400 seems to work better)\n",
        "\n",
        "# MelSpectogram params: convert raw data to frequency domain\n",
        "use_MFCC = False\n",
        "n_mfcc = 12 # input audio data size (higher -> more crisp sound) (e.g. 12)\n",
        "\n",
        "# Other data params\n",
        "number_of_samples = {'yes':2000, 'no': 2000} # We will try to classify between two classes. This defines the number of samples per class.\n",
        "noise = 0. # If not zero, it makes the audio proportionally noisier (range: 0-1)"
      ],
      "metadata": {
        "id": "m95VlOgmhuta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_data, audio_label = getData(sampling=sampling, use_MFCC=use_MFCC, n_mfcc=n_mfcc, number_of_samples=number_of_samples, noise=noise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psegm255h0S4",
        "outputId": "8253fe46-dbad-412f-c452-93396333399b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples: 3589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgHn5oNWQ98E",
        "outputId": "1fb1d3a5-1884-4179-9785-744c70f37fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3589, 400, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 440
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Playing an example"
      ],
      "metadata": {
        "id": "TwBNZm9sjqHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indx_to_play = 0\n",
        "\n",
        "# Only works if MFCC is not used\n",
        "print(audio_label[indx_to_play].item())\n",
        "ipd.Audio(audio_data[indx_to_play].squeeze(),rate=sampling,autoplay=True)"
      ],
      "metadata": {
        "id": "07nTwEDIkJJX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "37904cde-39a1-49a3-aaef-ec01d87a61c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
              "                    <source src=\"data:audio/wav;base64,UklGRkQDAABXQVZFZm10IBAAAAABAAEAkAEAACADAAACABAAZGF0YSADAAAlANf/HwAbAP7/6P8pAP3/9P8aANP/QQD6//v/VP+vAGj/pABF/8QAGv+WAFL/xQBX/48Amf+W/+0A2/2jAav/OgGl/8D/6/+lALX9cgKn+/0HUe44EWH5lvYEEoDvxA+G4Ksntukc8dAke9ZSIFrpXAwJB5/iUyoR0qYiuuyF+HYiFsdDQoy9ui/v6FH2yyd4vH5SCqUfVea9gB54CLDOW09/mcdtl5TEW0O+Sxx6DIPJNlqKi9Z+KYgIZZu3Cifi/ozXTVLqj/p7xoYvaxSpqDfb8ZDfXEUCmz52AYBQda6lGDQb8OrpYDzvnuh3WoQIcYykZTcS+DzRB1eDkBpzw5FIWdHK6v3nNwGf/HFQkEhYtdDw+fA7upzdbgClsC9z/vLWn0rGp/tCNed25Z866rujNF3vd+S8PWa6+imkBo7TSDkX0UkNUhyvxLI0EPI72qQ/CM12BWgp6Mf/HRMO8dDNLOv4W9rmNYroWOY0Lmvqdu/GJA3tTPRbGBPxSfrpDHH3OQLeANL8ogDQABYDovoOBJQBY/yoAcUAiv5jAsT+Fv6D/wADkP1Y/uECs/6p/3n+3AJx/4z/0wGb/9j/Lf+IAej/tv8AAX3+Nv9FAHP/i/5iADH/EwCD/+YAbwA3//gA+/+IADv/9//S/8//7f+B/8D/0f7qAIL/Tv9MAe3+Yv9E//UAM/+C/30BVwBp/ygARAFY/x0A2f+4ABj/cf9DAAAA1P+M/7gAeP+F/0//3ADx/5oALgBOAK0AHwBmALn+hQAs/zoACAC//2QAqP/nALv/3f8mAAIAQgAZAHUAYgAeAPP/mgDs/4L/7P/k/xUAPP8RAPb/vP8pAPD/dwBiAOX/+f8IAMH//v/2/xYAFwAsAEUAk/8PAEIA5v+2//7/9v+2/+H/6P8cANL/jAAoALb/DQDRAJUAtP+TANz/JQDO/zIAGADb/14A7/9KAK3/SwCC/7j/SgDp/9f/wP9KAO7//f8aAOb/+/8JAAMA9//1//L/zP/e/zMASgD0//T/6/8lAPX/NQAoAOn/HgDp/w4AKQCEAOv/y/8YAA==\" type=\"audio/wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {},
          "execution_count": 441
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model"
      ],
      "metadata": {
        "id": "UlP7n2lElCku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(dataloader, model):\n",
        "  acc = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for audios, labels in dataloader:\n",
        "          if use_cuda:\n",
        "            audios = audios.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "          outputs = model(audios)\n",
        "\n",
        "          # Update accuracy for this batch\n",
        "          acc = acc + torch.sum(torch.argmax(outputs, axis=1) == labels)\n",
        "\n",
        "\n",
        "      # Compute the accuracy\n",
        "      acc = acc/len(dataloader.dataset) # normalizes\n",
        "\n",
        "      return acc\n"
      ],
      "metadata": {
        "id": "QfscuLA3gqS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_loss(loader):\n",
        "  with torch.no_grad(): # Anything under torch.no_grad will be calculated with no gradients. Can only be used for testing, not training!\n",
        "\n",
        "    loss = 0\n",
        "    for i, (audios, labels) in enumerate(loader): # The batches.\n",
        "          # step1: Move data to cuda. Make sure the model is on cuda too!\n",
        "          if use_cuda:\n",
        "            audios = audios.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "          # step2: Forward pass\n",
        "          outputs = model(audios)\n",
        "\n",
        "          # step 3: calculate the loss.\n",
        "          loss = loss + audios.shape[0] * criterion(outputs, labels.view(-1))\n",
        "    return loss/ len(loader.dataset)"
      ],
      "metadata": {
        "id": "SWGM2CTe661x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the RNN classification model\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "      ######\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      ######\n"
      ],
      "metadata": {
        "id": "06JZIZFeIJcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "n4jUJYKIk8u7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next section, you will define your experiment's parameters and model's hyperparameters as flags here. Use these flags in your code so you can switch between experiemnts easily."
      ],
      "metadata": {
        "id": "iUhXt_QMmKkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# Data\n",
        "batch_size = 32 #(e.g., 32)\n",
        "\n",
        "# optimizer\n",
        "lr= 10e-4\n",
        "epochs=2000\n",
        "\n",
        "# model\n",
        "model_name = \"RNN\"\n",
        "hidden_size = 50 # hn for the RNN (e.g., 10)\n",
        "num_layers = 3 # Number of layers for RNN (e.g. 3)"
      ],
      "metadata": {
        "id": "dpIXwE4dk7ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vxsw0FHfZ2O",
        "outputId": "9a9a3db0-fb58-4588-efa5-28e31d9ee528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3589, 400, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 489
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "fnJHcf0jldqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "def getDataLoaders(audio_data, audio_label, batch_size):\n",
        "  # # normalization\n",
        "  audio_data_mean = audio_data.mean(1).unsqueeze(1)\n",
        "  audio_data_std = audio_data.std(1).unsqueeze(1)\n",
        "  audio_data_transformed = (audio_data - audio_data_mean)/ audio_data_std\n",
        "\n",
        "  hist = torch.histogram(audio_data_transformed)\n",
        "\n",
        "  plt.plot(hist.bin_edges[:-1], hist.hist, color=\"r\")\n",
        "\n",
        "\n",
        "  # Load the data loaders\n",
        "  my_dataset = torch.utils.data.TensorDataset(audio_data_transformed, audio_label)\n",
        "  train_set, val_set = torch.utils.data.random_split(my_dataset, [0.8, 0.2])\n",
        "\n",
        "  train_loader = DataLoader(dataset=train_set, batch_size=batch_size,  shuffle=True)\n",
        "  val_loader = DataLoader(dataset=val_set, batch_size=batch_size,  shuffle=False)\n",
        "\n",
        "  return train_loader, train_set, val_loader, val_set"
      ],
      "metadata": {
        "id": "LJw3mv-blhFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, train_set, val_loader, val_set = getDataLoaders(audio_data, audio_label, batch_size)"
      ],
      "metadata": {
        "id": "pzRjvKvFhusE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0][0].shape"
      ],
      "metadata": {
        "id": "msPwdpjyQUaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "5MEEKaylltRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchinfo"
      ],
      "metadata": {
        "id": "uJjkVm7etf9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchinfo import summary\n",
        "\n",
        "# Create an instance of the RNN classification model\n",
        "input_size = train_set[0][0].shape[-1]\n",
        "num_classes = 2\n",
        "model = #####\n",
        "print(model)\n",
        "print(summary(model, train_set[0][0].unsqueeze(0).shape))\n",
        "\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.cuda()\n",
        "\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.9) #(e.g., factor=0.5)\n",
        "\n",
        "for epoch in range(epochs): # The epochs.\n",
        "    for i, (audios, labels) in enumerate(train_loader): # The batches.\n",
        "        # step 1: Zero out the gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # step 1.1 move data to cuda. Make sure the model is on cuda too!\n",
        "        if use_cuda:\n",
        "          audios = audios.cuda()\n",
        "          labels = labels.cuda()\n",
        "\n",
        "        # print('labels', labels)\n",
        "\n",
        "\n",
        "        # step2: Forward pass\n",
        "        outputs = model(audios)\n",
        "\n",
        "        # print('outputs', outputs)\n",
        "\n",
        "        # step 3: calculate the loss.\n",
        "        loss = criterion(outputs, labels.view(-1))\n",
        "\n",
        "        # step 4: Backward pass\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        #Print the loss\n",
        "        if epoch %1 == 0 and i %300 == 0:\n",
        "          print(\"Epoch\", epoch+ 1, \" batch\", i+1, \". Training Loss: \", loss.item())\n",
        "\n",
        "            # Print the loss\n",
        "    scheduler.step(get_loss(val_loader))\n",
        "    if epoch %1 == 0:\n",
        "      print(\"Epoch\", epoch+ 1,  'acc: ', get_accuracy(train_loader,model).item(), 'val_acc: ', get_accuracy(val_loader,model).item())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sGjuLFl0TTYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have a trained model and a DataLoader for test data\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Initialize empty lists for predictions and ground truth labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:  # Replace 'dataloader' with your actual DataLoader\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)  # Get predicted class labels\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)  # Replace 'class_names' with your class labels\n",
        "disp.plot(cmap=plt.cm.Blues, values_format=\".0f\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Br2cHjUMoXfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}