{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamod/BA865-2024/blob/main/hands-on/Draft_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, that you know how to train an MLP, let's make minor changes to train a"
      ],
      "metadata": {
        "id": "W8H546tNarSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Things we will investigate:\n",
        "\n",
        "\n",
        "- How to fine-tune a pre-trained model.\n",
        "  - How to freeze layers.\n",
        "  - How to train the model or a part of it.\n",
        "- How to load data from disk.\n",
        "- How to do image transformations.\n",
        "- How to use LR scheduler\n"
      ],
      "metadata": {
        "id": "Gt1GCe9Kuzee"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OmtINP6HzDps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some flags that you can turn on and off"
      ],
      "metadata": {
        "id": "fURxa8CNTnp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enable_wandb = True\n",
        "use_gpu = True\n"
      ],
      "metadata": {
        "id": "QnfvDHZOTk8N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import some packages"
      ],
      "metadata": {
        "id": "frEXdeTWNXqt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-8PrSb5Idis"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This helps you check if GPU is available"
      ],
      "metadata": {
        "id": "0BNuG94i373o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_available = torch.cuda.is_available()\n",
        "gpu_available"
      ],
      "metadata": {
        "id": "Zd7mgN-abpJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some extra fancy but optional packages:\n",
        "\n",
        "- `torchmetrics` for calculating accuracy\n",
        "- `wandb` for logging"
      ],
      "metadata": {
        "id": "9nYYpgwD3_-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if enable_wandb:\n",
        "  !pip install wandb -qU\n",
        "  import wandb\n",
        "  wandb.login()"
      ],
      "metadata": {
        "id": "e8unrZajZigs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper-parameters\n",
        "\n",
        "Define your hyper-parameters here."
      ],
      "metadata": {
        "id": "dJUP7ISFNbHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "configs = {\n",
        "    experiment_name: \"Default\"\n",
        "    freeze_feature_extraction_layers: False\n",
        "    pretrained:True\n",
        "    efficientnet:True\n",
        "    data_augmentation:True\n",
        "\n",
        "    # Data\n",
        "    img_dimensions : (3,224,224)\n",
        "    batch_size : 32\n",
        "    num_classes : 14\n",
        "\n",
        "    # CNN\n",
        "    filter_sizes : [4,8]\n",
        "    kernel_size : 5\n",
        "    stride: 2\n",
        "    padding:1\n",
        "\n",
        "    #Optimzation\n",
        "    learning_rate : 0.01\n",
        "    epochs : 70\n",
        "    weight_decay : 0.00001\n",
        "}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7euepcWJX1Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "Load your dataset and create `DataLoaders` that handle the batching and shuffling.\n",
        "\n",
        "Let's use this dataset: https://www.kaggle.com/datasets/danupnelson/14-celebrity-faces-dataset?resource=download"
      ],
      "metadata": {
        "id": "VWFOwqDtNeG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip \"/content/archive (10).zip\""
      ],
      "metadata": {
        "id": "uy-u4xOmkM_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "if configs.data_augmentation:\n",
        "  transform = transforms.Compose([\n",
        "#####\n",
        "  ])\n",
        "else:\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Resize(224),       # Resize the image to 224x224 pixels while maintaining aspect ratio\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),               # Convert the image to a PyTorch tensor\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize the tensor using the mean and\n",
        "                      std=[0.229, 0.224, 0.225])       # standard deviation of the ImageNet dataset\n",
        "  ])\n",
        "\n",
        "\n",
        "import torchvision\n",
        "#####\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=configs.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=configs.batch_size, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=configs.batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "wTgFHFDMJTe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define and create your model"
      ],
      "metadata": {
        "id": "O43tOiHANfHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, output_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            torch.nn.Conv2d(configs.img_dimensions[0], configs.filter_sizes[0], configs.kernel_size, stride=configs.stride, padding=configs.padding),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(configs.filter_sizes[0], configs.filter_sizes[1], configs.kernel_size, stride=configs.stride, padding=configs.padding),\n",
        "            torch.nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(72, output_size) # We can determine the 72 here by doing calculations offline or looking at the error.\n",
        "        )\n",
        "\n",
        "    # Defines the forward pass.\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, configs.img_dimensions[0], configs.img_dimensions[1], configs.img_dimensions[1])\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "dVLVk-DmJVFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model"
      ],
      "metadata": {
        "id": "R9N6B87Y4pO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "if configs.transferlearning:\n",
        "  # Load a pre-trained ResNet-18 model\n",
        "  if not configs.efficientnet:\n",
        "\n",
        "  else:\n",
        "\n",
        "else:\n",
        "  model = CNN(configs.num_classes)\n",
        "\n",
        "if gpu_available and use_gpu:\n",
        "  model = model.cuda()\n"
      ],
      "metadata": {
        "id": "eRrS3D6gJVHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "Pc2kqfSdfUaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace the last layer(s)"
      ],
      "metadata": {
        "id": "JaGghG3Ozi6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if configs.transferlearning:\n",
        "  if configs.efficientnet:\n",
        "    model.classifier[1] = torch.nn.Sequential(torch.nn.Linear(1408, 100), torch.nn.ReLU(), torch.nn.Linear(100, configs.num_classes))\n",
        "  else:\n",
        "    model.fc = torch.nn.Sequential(torch.nn.Linear(512, 100), torch.nn.ReLU(), torch.nn.Linear(100, configs.num_classes))\n",
        "model"
      ],
      "metadata": {
        "id": "Cf7mJ4PCf9bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freeze other layers"
      ],
      "metadata": {
        "id": "k5SHByvJzlUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if configs.transferlearning:\n",
        "  if configs.freeze_feature_extraction_layers==True:\n",
        "    # Freeze all layers except the last fully connected layer\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Unfreeze the last fully connected layer\n",
        "    if configs.efficientnet:\n",
        "      for param in model.classifier[1].parameters():\n",
        "          param.requires_grad = True\n",
        "    else:\n",
        "      for param in model.fc.parameters():\n",
        "          param.requires_grad = True"
      ],
      "metadata": {
        "id": "XjhF8ryLg8Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "use `torch-summary` for more info on the model"
      ],
      "metadata": {
        "id": "_73Cq5Cf7-wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "HpDl2R-H8GGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(model, (1, 3, 224, 224))"
      ],
      "metadata": {
        "id": "-ZOWbjKT7-4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss\n",
        "\n",
        "For classification, we use cross-entropy."
      ],
      "metadata": {
        "id": "RiPIJNAaNirU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "S4LHoaZaJXd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Optimizer"
      ],
      "metadata": {
        "id": "PtHuHVcSNjrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), momentum=0.9, lr=configs.learning_rate, weight_decay=configs.weight_decay)"
      ],
      "metadata": {
        "id": "HYtLekcnJYgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training!"
      ],
      "metadata": {
        "id": "slsVWSc_Nk6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if enable_wandb:\n",
        "  wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=\"Transfer Learning\",\n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=configs.experiment_name,\n",
        "    # Track hyperparameters and run metadata\n",
        "    config=configs)"
      ],
      "metadata": {
        "id": "8MrmIuolgDFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some functions to calculate training and test accuracies"
      ],
      "metadata": {
        "id": "hvVft-hIuObz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## There is a package called torchmetrics that makes calculating accuracy easier.\n",
        "## Feel free to use it. However, here, I show the logic behind such calculations\n",
        "# !pip install -U torchmetrics\n",
        "# import torchmetrics\n",
        "\n",
        "def get_accuracy(dataloader, model):\n",
        "  acc = 0\n",
        "  # <OR>\n",
        "  # acc = torchmetrics.Accuracy()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for images, labels in dataloader:\n",
        "          if gpu_available and use_gpu:\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "          outputs = model(images) # get predictions\n",
        "\n",
        "          # Update accuracy for this batch\n",
        "          acc = acc + torch.sum(torch.argmax(outputs, axis=1) == labels)\n",
        "          # <OR>\n",
        "          # acc.update(outputs, labels)\n",
        "\n",
        "\n",
        "      # Compute the accuracy\n",
        "      acc = acc/len(dataloader.dataset) # normalizes\n",
        "      # <OR>\n",
        "      # acc = acc.compute()\n",
        "\n",
        "      return acc\n",
        "\n"
      ],
      "metadata": {
        "id": "N-RG6HEnuOga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(loader):\n",
        "  with torch.no_grad(): # Anything under torch.no_grad will be calculated with no gradients. Can only be used for testing, not training!\n",
        "\n",
        "    loss = 0\n",
        "    for i, (images, labels) in enumerate(loader): # The batches.\n",
        "          # step1: Move data to cuda. Make sure the model is on cuda too!\n",
        "          if gpu_available and use_gpu:\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "          # step2: Forward pass\n",
        "          outputs = model(images)\n",
        "\n",
        "          # step 3: calculate the loss.\n",
        "          loss = loss + criterion(outputs, labels)\n",
        "    return loss/ len(loader)"
      ],
      "metadata": {
        "id": "EyMtSliNzyrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early-stopping class"
      ],
      "metadata": {
        "id": "maJvkRGjy7Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopper:\n",
        "    def __init__(self, patience=1):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = float('inf')\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        # If the new loss is lower than the old loss, reset the counter!\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "            # Keep track of the best model by saving it on the hard drive.\n",
        "            torch.save(model.state_dict(), \"./best_model.pt\")\n",
        "        # otherwise, increment the counter.\n",
        "        elif validation_loss > self.min_validation_loss:\n",
        "            self.counter += 1\n",
        "            # If there has been too many epochs with the loss being high, terminate.\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "ObOAcccny7Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train!"
      ],
      "metadata": {
        "id": "pOwDP6qQuUMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopper = EarlyStopper(patience=7)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=3)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(configs.epochs): # The epochs.\n",
        "    for i, (images, labels) in enumerate(train_loader): # The batches.\n",
        "        # step 1: Zero out the gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # step 1.1 move data to cuda. Make sure the model is on cuda too!\n",
        "        if gpu_available and use_gpu:\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "\n",
        "        # step2: Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # step 3: calculate the loss.\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # step 4: Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print the loss\n",
        "        if i %100 == 0:\n",
        "          print(\"Epoch\", epoch+ 1, \" batch\", i+1, \". Training Loss: \", loss.item())\n",
        "          if enable_wandb:\n",
        "            wandb.log({\"loss\": loss})\n",
        "\n",
        "    # Compute total train accuracy\n",
        "    train_acc = get_accuracy(train_loader, model)\n",
        "    test_acc = get_accuracy(val_loader, model)\n",
        "\n",
        "    validation_loss = get_loss(val_loader)\n",
        "    wandb.log({\"val_loss\": validation_loss})\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], LR: {optimizer.param_groups[0][\"lr\"]}, Validation Loss: {validation_loss.item():.4f}, Train Accuracy: {train_acc.item():.4f}, Validation Accuracy: {test_acc.item():.4f}')\n",
        "    if enable_wandb:\n",
        "      wandb.log({\"epoch\": epoch + 1, \"LR\": {optimizer.param_groups[0]['lr']}, \"train_accuracy\": train_acc.item(), \"val_accuracy\": test_acc.item()})\n",
        "\n",
        "    scheduler.step(validation_loss)\n",
        "\n",
        "    if early_stopper.early_stop(validation_loss):\n",
        "        print(\"Validation loss hasn't dropped. Early stopping!\")\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "id": "wNhVx_muJcfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "7WOyv2ABY_30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"last model performance on Test set:\", get_accuracy(test_loader, model))\n",
        "\n",
        "# Once training is done, load the best model (might not be the last model due to early stopping)\n",
        "model.load_state_dict(torch.load(\"./best_model.pt\"))\n",
        "acc = get_accuracy(test_loader, model)\n",
        "print(\"best model performance Test set:\", acc)\n",
        "\n",
        "if enable_wandb:\n",
        "  wandb.summary['Test Accuracy'] = acc.item()"
      ],
      "metadata": {
        "id": "SZcL-YL-YcRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "TLhvpPZ2Nmep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if enable_wandb:\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "eWlDbw_cacL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "        output = model(inputs.cuda()) # Feed Network\n",
        "\n",
        "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
        "        y_pred.extend(output) # Save Prediction\n",
        "\n",
        "        labels = labels.data.cpu().numpy()\n",
        "        y_true.extend(labels) # Save Truth\n",
        "\n",
        "# constant for classes\n",
        "classes = test_dataset.class_to_idx.keys()\n",
        "\n",
        "# Build confusion matrix\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
        "                     columns = [i for i in classes])\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (12,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.savefig('output.png')"
      ],
      "metadata": {
        "id": "yrSodcyVeDwb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}